---
layout: post 
category: papers 
---
---
为了优化小容量键值对无法充分使用持久化内存（persistent memory,PM）带宽的问题，提出了FlatStore。将传统键-值存储解耦为以高效存储为目的的持久化日志结构和以快速索引为目的的非持久化索引。

- 压缩日志结构以提高并行性
- 流水线水平批处理，在创建批处理时从其他核心窃取日志条目，从而提供低延迟和高吞吐量性能

---

重点：**如何设计高效的键值存储来处理写密集型和小型的工作负载**

问题：当前很多种kv存储访问模式的粒度和持久化内存不匹配。当前的kv存储系统会生成巨多的小体积写，很大比例的k-v对包含很小的应用工作负荷（10字节左右）。此外，kv存储的索引结构导致了巨大的写放大。CPU以cacheline的粒度（64B in AMD adn x86 platform）持久化数据，但是PM的内部块(256 B in Optane DCPMM)更大，显著浪费了硬件带宽。

本文在傲腾持久化内存上部署了一个持久化的KV索引

经典的方法是使用日志化结构组织kv存储，然后所有更新只需要添加到日志，还可以批处理来自客户端的多个请求分摊开销。这个想法在HDD和SSD上运行地很好因为他们有很好的顺序写性能，每一次写的吞吐量可以很大（up to tens of MBs)。但是在NVM-based系统上不太行：

- 只要I/O大小比最小的I/O单元（i.e. 256B block size）大，并且有足够多的并发I/O，傲腾持久内存有非常相似的访问速度，所以无法通过批处理存储器访问来提高性能。并且持久化内存的访问粒度更小（i.e., 64 B flush size and 256 B block size），如果我们把所有对存储器的更新都放在日志里面，那么一次访问内存只能得到很有限的日志条目。

- 批量处理有明显的延迟，当前的非易失性内存和高速网卡都主打低延迟，那么传统的批量处理就不太合适。对多核处理器也不友好

<font color="grey">我的理解：leveldb在内存中维护一个跳表，直到它足够大就dump到磁盘中实现持久化，这样做可以将批量的小磁盘I/O转化为一个大的连续写，这样对于机械硬盘和传统SSD是有加速效果的，但是对于NVM没什么效果，而且还会导致延迟，所以作者要找一个新方法。</font>

将kv存储分为两个部分：

- 易失性的索引（为了提高索引速度）
- 持久化日志结构（为了提高存储效率）

**紧凑日志格式**和**流水线水平批处理**实现高吞吐量，低延迟和多核可扩展性

---

当今工业上对数据库的需求从读密集转向写密集，并且大部分数据库条目并不大。

对数据库的小更新和持久化数据的粒度不匹配

傲腾可持久化内存的几个特点

- 顺序和随机访问在高并发的条件下有相似的带宽
- 重复持久化到相同的缓存行有巨大延迟（对于就地更新不友好）
  - clwb指令是异步执行的，后面的clwb指令要等前面的执行完
  - 傲腾内存的片上防止磨损机制可能阻止了同一个缓存行上后续的数据覆写

---

FlatStore 的设计原则

- 最小化写开销
- 低延迟
- 多核可拓展性

![image-20211003112145878](../../www/assets/pic/image-20211003112145878.png)

核心组件：

- 压缩的操作日志（Compacted OpLog）FlatStore incorporates a per-core log-structured OpLog to absorb frequent and small-sized updates with efficient batching.

- 惰性持续分配器（Lazy-persist Allocator）。为存储大型kv对打造，由于OpLog已经记录了大型kv的地址，不再需要持久化相同的元数据（位图）。

- 流水线水平批处理（Pipelined Horizontal Batching）。允许核心在创建批处理时从其他核心窃取日志条目

为了加速检索，在DRAM中存储额外的易失性索引副本

---

**Compacted OpLog and Lazy-Persist Allocator**

如前所述，在持久键值存储中更新通常会导致巨大的写入放大和缓存冲洗。

<font color="grey">什么是缓存冲洗（flush）？ 当高速缓存中的未写入数据量达到一定级别时，控制器周期性地将高速缓存的数据写入驱动器。 这个写入过程称为“冲洗”。 控制器使用两个算法进行持久化缓存：基于需求和基于年龄的算法</font>

为解决这个问题，FlatStore将kv存储拆分为：

- 易失性索引
- 压缩的、每个核心的OpLog处理小更新，持久化分配器存储大kv对

当执行一个更新请求时，处理器核心仅写键值，将一个日志条目加到它的本地日志末尾然后更新易失性索引。因此，避免了散列索引重新哈希或者树形索引移位/分裂/合并而引起的持久化开销。要服务于GET请求，服务器核心首先引用带给定键的易失性索引来查找键值项以查找确切的日志条目，最后得到键值项。以这种方式，扫描整个日志以找到特定键的开销也被避免。

**日志条目的压缩**

Oplog的一个关键设计方面是如何构建日志条目的布局：日志条目应该设计足够小以更好地支持批处理，使得可以在一次flush操作中持久化更多的条目。因此，仅将索引元数据和极小的kv项放到OpLog中（在我们的实现中，256 B足以使Optane DCPMM的带宽饱和）然后其他的大型kv项目由一个分配器单独存储。同时，每个日志条目还需要在运行时包含足够的元数据以供运行期间的正常索引并且在系统崩溃后安全地恢复丢失的易失性索引结构。

日志条目不记录存储器的每一次更新，仅记录最小的描述每一次操作的信息。

![image-20211003204738291](../../www/assets/pic/image-20211003204738291.png)

Op记录操作类型（是Put还是Delete），Emd表示k-v对是否放置在日志末尾，Version确保日志清理时的正确性。key占8字节（FlatStore可以通过将键放在OpLog中以支持更大的键），Ptr指向实际存储值的位置（Ptr占用40比特，由于分配器只管理大于256字节的大型kv对，表示地址的低8位比特可以省略，40+8比特的地址可以覆盖128TB的NVM空间，所以这是可以接受的）<font color="grey">所以实际存储值的存储块是向$2^8$字节对齐的</font>。所以每一个基于指针的日志条目被限制为16字节，所以可以一次性持久化16个这样的日志条目，代价和持久化一条日志是相同的。

**填充**

大部分情况都避免将数据持久化到同一个位置（同一个cacheline），所有数据都是异地更新的。但是两个相邻的批次仍可能在OpLog中共享同一个caheline，因为批次的大小不一定总是能刚刚好（如上一段所说的16条日志）。这种情况会导致延迟。解决方法是为不够大的批次填充一部分字节。就地更新是很难处理的，尤其是工作负载发生倾斜的时候（<font color="grey">例如，在社交媒体网站上，如Twitter、微博等，一个拥有数百万追随者的名人用户在做某事时可能会引发一场风暴。这个事件可能导致大量写入同一个键（键可能是名人的用户ID，或者人们正在评论的动作的ID）。</font>）

**懒持久化分配器**

使用分配器存储大的键值对会导致额外的持久化开销，因为NVM分配器也需要小心地保存他私有的元数据，追踪NVM中那些地址是被使用的而哪些是空的。我们会观察到持久化日志项和分配器元数据之间有冗余的存储器写入，因此，我们可以利用这些冗余信息，推迟持久化分配器元数据或者在系统发生故障时正确地恢复。主要的挑战是如何在恢复期间在每个日志条目中使用Ptr来反向定位分配元数据，因为需要使用分配器来支持变长分配

假设一种懒持久化分配器，他最初将NVM空间切成4MB的块。然后将4MB的块切割成不同种类的数据块，并且原本归属于同一个大块的小内存块有相同的大小。当每个NVM块准备分配时，这样的切割大小会持续记录在它的头部。位图也放置在每个块的头部，以跟踪未使用的数据块。在这种设计下，每个块的起始地址是4mb对齐的，每个块的分配粒度在其头部指定。因此，一个已分配的NVM块在大块中的偏移量可以通过有效日志条目中的Ptr直接计算出来，这样即使在系统崩溃前无法持久化位图，我们也可以恢复位图。考虑到可伸缩性问题，这4mb的NVM块被划分到不同的服务器核心，然后通过修改位图(不写数据)分配一个空闲数据块。对于大于4 MB的分配(在键值存储中不太可能发生这种情况)，我们直接用一个或多个连续的块分配它。



---

流水线水平批处理

垂直批处理指的是每个服务器核心只能处理它收到的请求，流水线水平批处理可以让服务器核心取得其他核心的等待被持久化的日志条目。
